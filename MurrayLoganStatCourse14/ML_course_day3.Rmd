
Day 3 - Bayesian
==================
Credit for this script goes to: Murray Logan (main script), and Lotte van Boheemen (edits)

```{r}
setwd("~/Documents/Monash/PhD/R/MurrayLoganStatCourse14")
```

With frequentist it is almost impossible *not* to disprove the null-hypothesis, as the probability of the data is calculated from the data. We are not interested in the sample, we are interested in the population. 

```
                   *Frequentist*       *Bayesian*
Observed data     One possible          Fixed, true
Parameters        Fixed, true           Random, distribution
Inferences        Data                  Parameters
Probability       Long-run frequence    Degree of belief
                    $(P|H)$               $P(H|D)$
```

With the Frequentist approach you get one point value for the slope, with the Bayesian you get a distribution. When 90% of this distribution lies above or below zero, you can consider that there is an effect in the data. 


*Bayes rule*

*P*(*H*|*D*) = (*P*(*D*|*H*) * P(*H*)) / *P*(*D*)

posterior probability = ( likelyhood x prior probability) / normalizing constant

Propability distribution: area under curve must be 1
Likelihood distribution: are under curve is unknown

Above formula normalizes the area under the curve and makes it equal to 1

**Bayesian is robust to:**

> unequal sample sizes

> balance

> collinearity

> pair-wise comparisons (no need for *post-hoc* tests)


**MCMC-Sampling**

Random walks on distribution. Will always accept a position with a higher probability and whether or not a lower probability point is accepted depends on the actual likelihood of that point. For this reason, we will get a better overview of more probable parts of the distribution. 
This chain of walks is called a chain!
- Run 3 chains to make sure this is the distribution
- Burning: run for example for a 1000 steps (50% of the actual iterations you want to run) and see what an appropriate distance per step is. After this burning, chuck this burning run out.
- Number of iterations: determined by diagnostics that are run

Start with 10000 iterations and make trace plots:
- needs to be total chaos, when not, include more iterations
- needs to be repeatable

Autocorrelation
- ACF plot
- thinning, thinning factor 10 means only sample every 10th point. When thinning, make sure to increase the number of iterations.

Distribution plots
- multiple modes --> not one single population sampled
- highly skewed --> take median or mode
- area under curve is 1
- credibility intervals (same as confidence interval)

MCMCglmm is not too highly developed in R... Alternative is WinBUGS or JAGS (C++). These programs use a Gibbs sampler. This changes one parameter at a time (for example first intercept, then slope). Better samplers exist, one of which is implemented in STAND. 

In C++ not everything has to be scripted in the same order, but everything **will be run simultaneously**

JAGS can be run through other platforms requires and 3 inputs:
- file with model statement
- data in the form of a list
- parameters that define the run (number of iterations, burning etc)

From R: R2Jags

Start as simple as possible, get something to compare to
```{r}
fert <- read.csv('fertilizer.csv',strip.white=T)
summary(lm(YIELD~FERTILIZER,data=fert))
```

```{r,include=FALSE}
library('MCMCpack')
```

To get the contents of the package, type `MCMCpack:::`

This package only runs one chain at the time, so it should be run 3 times
```{r}
fert.mcmc1 <- MCMCregress(YIELD~FERTILIZER, data=fert,
                          burnin = 100, mcmc =1000, thin=1,seed=1)
```

Seed: the random seed, which dictates how the randomization works. So if you want to run another random run, select another seed:

```{r}
fert.mcmc2 <- MCMCregress(YIELD~FERTILIZER, data=fert,
                          burnin = 100, mcmc =1000, thin=1,seed=2)
fert.mcmc3 <- MCMCregress(YIELD~FERTILIZER, data=fert,
                          burnin = 100, mcmc =1000, thin=1,seed=3)
```

Now merge them together so you have one object which you can process together. 
```{r,include=FALSE}
library('coda')
```
```{r}
fert.mcmc <-as.mcmc.list(list(fert.mcmc1,fert.mcmc2,fert.mcmc3))
```

Now take advantage of things that are within coda, like the plotting function
```{r}
plot(fert.mcmc)
```

> Top row: intercept
> Middle row: slope
> Bottom row: variance

Colours of the lines, black, green and red (printed in that order) are the 3 runs. You want these runs to show the exact same. These are, so it's likely you have transfersed the entire prosterior. 

Notice that the slope and intercept are fairly normal, so to describe it a mean would be OK. The variance is skewed so to describe it a mode or median would be better.

We used a thinning of 1, which is a lag of one in the next output:
```{r}
autocorr.diag(fert.mcmc)
```
We don't want to see high numbers in this plot. A high number is something higher that .2. We have one sigma2 value which is greater than .2, so we have quite a bit of correlation. So we need to re-run the chains with a different thinning factor. Now this output is obvious. Note that sometimes this autocorrelation loops back to higher value. 

Let's try a thin of 5 (now need to increase the number of iterations 5 times)

```{r}
fert.mcmc1 <- MCMCregress(YIELD~FERTILIZER, data=fert,
                          burnin = 100, mcmc =5000, thin=5,seed=2)
fert.mcmc2 <- MCMCregress(YIELD~FERTILIZER, data=fert,
                          burnin = 100, mcmc =5000, thin=5,seed=2)
fert.mcmc3 <- MCMCregress(YIELD~FERTILIZER, data=fert,
                          burnin = 100, mcmc =5000, thin=5,seed=3)
fert.mcmc <-as.mcmc.list(list(fert.mcmc1,fert.mcmc2,fert.mcmc3))
plot(fert.mcmc)
autocorr.diag(fert.mcmc)
```

Now all looks good.

There is another diagnostic, the raftery diagnostic. Looks if you had enough samples in your chains
```{r}
raftery.diag(fert.mcmc)
```

So we need at least 3746 samples in our chains, we had 5000, so yay.

Now summarize the data:
```{r}
summary(fert.mcmc)
```

We can now make the statement that the slope is between 0.62 and 1.0 with 95% probability. We CANNOT make the same statement with 95% confidence intervals.

```{r}
fert.mcmc <- rbind(fert.mcmc1, fert.mcmc2,fert.mcmc3)
head(fert.mcmc)
```

We now have 3000 estimates of slopes (1000 per run).

We can now make a histogram with the distribution of the slopes.
```{r}
par(mfrow=c(1,2))
hist(fert.mcmc[,2])
```

The estimate of the slope is the mean or the median:
```{r}
mean(fert.mcmc[,2])
```

And the credibility intervals are:
```{r}
HPDinterval(as.mcmc(fert.mcmc[,2]))
```

These intervals are density based, and don't have to be equal around the mean.

In library(plyr)
```{r}
fert.sum <- adply(fert.mcmc,2, function(x) {
  data.frame(Mean=mean(x), Median=median(x),HPDinterval(as.mcmc(x)))
})
```

In this formula, the 2 refers to the columns. 
```{r}
head(fert.sum)
```

We can now see how many times the estimate of the slope was greater than 0:count up the number of slopes greater than zero and devide this by 3000 (for the total number of iterations):
```{r}
table(fert.mcmc[,2]>0)
table(fert.mcmc[,2]>0)/length(fert.mcmc[,2])
```

We can now also test our distribution agains any hypothesis (e.g. the slope is .8):
```{r}
table(fert.mcmc[,2]>0.8)
table(fert.mcmc[,2]>0.8)/length(fert.mcmc[,2])
```

In JAGS/BUGS, things are written in terms of precision, not variance.

Model:
y(i) ~ N(mu(i),tau)
mu(i) = beta(0) + beta(1*x(i))

We are going to use some non-informative priors:

> beta(0) ~ *N*(0,0.000001) : the intercept is 0 with a very high uncertaincy
> beta(1) ~ *N*(0,0.000001) : the slope is 0 with a very high uncertaincy
> sigma ~ *Uniform*(0,100)

The model is (by using R to write the text file for you)
```{r}
modelString="
model {
  #Likelihood
  for (i in 1:n) {
    y[i]~dnorm(mu[i],tau)
    mu[i] <- beta0+beta1*x[i]
  }
  
  #Priors
  beta0 ~dnorm(0,1.0E-6)
  beta1 ~dnorm(0,1.0E-6)
  tau <- 1/ (sigma * sigma)
  sigma~dunif(0,100)
}
"
writeLines(modelString,con="regression.txt")
```

Now we need to create a list with out data in it. The above model is expecting x, y and n (number of rows)
```{r}
fert.list <- with(fert,
                  list(x=FERTILIZER,
                  y=YIELD,n=nrow(fert))
                  )
```

Now add the parameters
```{r}
params<- c("beta0","beta1","sigma")
```

Now put it in Jags
library(R2Jags)
Use the chache=TRUE command in the {r} for knitr, so it stores the data and it doesn't have to re-run every time you run knitter. Do this for any large amounts of data
```{r runJags3,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
fert.r2jags <- jags(data=fert.list,
                    inits=NULL,
                    parameters.to.save=params,
                    model.file="regression.txt",
                    n.chains=3,
                    n.iter=1000,
                    n.burnin=100,
                    n.thin=5
                    )
```

In the output, the first line represents the burnin, the second one the iterations.
This has now come back from jags and we need to run diagnostics

`quartz()` can be used if figure margins are too large

```{r}
plot(as.mcmc(fert.r2jags))
```

We are not too happy with the trace plots so re-run with more iterations and higher burnin

```{r,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
fert.r2jags2 <- jags(data=fert.list,
                    inits=NULL,
                    parameters.to.save=params,
                    model.file="regression.txt",
                    n.chains=3,
                    n.iter=10000,
                    n.burnin=1000,
                    n.thin=5
                    )
plot(as.mcmc(fert.r2jags2))
```

Auto-correlation plot:
```{r}
autocorr.diag(as.mcmc(fert.r2jags2))
```

```{r}
print(fert.r2jags)
```

In this, you want n.eff to be at least half of the number of iterations. Because there is no use of calculating the mean slopes etc when you don't have a high enough number of **effective** estimates.

The model shown before was the bare minimum required. Let's make it a little more complicated. We are now storing the residuals (y.err) and something that test  the probability of the slope being greater than 0.
```{r}
modelString="
model {
  #Likelihood
  for (i in 1:n) {
    y[i]~dnorm(mu[i],tau)
    mu[i] <- beta0+beta1*x[i]
    y.err[i] <- x[i] - mu[i]
  }
  
  #Priors
  beta0 ~dnorm(0,1.0E-6)
  beta1 ~dnorm(0,1.0E-6)
  tau <- 1/ (sigma * sigma)
  sigma~dunif(0,100)
  p.increase <- step(beta1)
  sd.x <- abs(beta1)*sd(x[])
  sd.resid <- sd(y.err)
}
"
writeLines(modelString,con="regression2.txt")

fert.list <- with(fert,
                  list(x=FERTILIZER,
                  y=YIELD,n=nrow(fert))
                  )
```

We need to change the parameters, because we want more to be returned to us
```{r}
params<- c("beta0","beta1","sigma","p.increase","sd.x","sd.resid")
```

```{r}
burnInSteps = 2000
nChains = 3
numSavedSteps = 50000
thinSteps = 5
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
nIter
```

```{r runJags2,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
fert.r2jags3 <- jags(data=fert.list,
                    inits=NULL,
                    parameters.to.save=params,
                    model.file="regression2.txt",
                    n.chains=nChains,
                    n.iter=nIter,
                    n.burnin=burnInSteps,
                    n.thin=thinSteps
                    )
```

```{r printJags}
#print(fert.r2jags3)
```

Start making some plots.
First convert to mcmc output:
```{r}
fert.mcmc <- as.mcmc(fert.r2jags$BUGSoutput$sims.matrix)
head(fert.mcmc)
```

First step, identify the columns in which the steps and intercept are in: bet0 and beta1, first 2 columns.
We now have 50000 coefficients for each the slope and the intercept. 

```{r}
coefs <- fert.mcmc[,1:2]
xs <- seq(min(fert$FERTILIZER), max(fert$FERTILIZER),len=100)
Xmat <- model.matrix(~FERTILIZER,data=data.frame(FERTILIZER=xs))
head(Xmat)
pred <- coefs %*% t(Xmat)
dim(pred)
```

Now calculate the average of all the predictions
```{r}
#newdata <- adply(pred,2,function(x){
#  data.frame(Mean=mean(x),Median=median(x),HPDinterval(as.mcmc(x))
#})
#newdata <- cbind(newdata,FERTILIZER=xs)
#head(newdata)
```

Now do the plotting
```{r}
#ggplot(newdata,aes(y=Mean,x=FERTILIZER)) + geom_line()
#  geom_ribbon(aes(ymin=lower,ymax=upper),fill='blue', alpha=.2) +
# theme_classic()
```


```{r}
loyn <- read.csv('loyn.csv',strip.white=T)
head(loyn)
loyn$logAREA <- log10(loyn$AREA)
loyn$logDIST <- log10(loyn$DIST)
loyn$logLDIST <- log10(loyn$LDIST)
summary(lm(ABUND~logAREA+logDIST+logLDIST+GRAZE+ALT+YR.ISOL,data=loyn))
```

```{r}
modelString="
model {
  #Likelihood
  for (i in 1:n) {
    abund[i]~dnorm(mu[i],tau)
    mu[i] <- beta0+beta.dist*(dist[i])+beta.ldist*(ldist[i]) +
      beta.area*(area[i]) + beta.graze*(graze[i]) + 
      beta.alt*(alt[i]) + beta.yr*(yr[i])
    }
  
  #Priors
  beta0 ~dnorm(0,1.0E-6)
  beta.dist ~dnorm(0,1.0E-6)
  beta.ldist ~dnorm(0,1.0E-6)
  beta.area ~dnorm(0,1.0E-6)
  beta.graze ~dnorm(0,1.0E-6)
  beta.alt ~dnorm(0,1.0E-6)
  beta.yr ~dnorm(0,1.0E-6)
  tau <- 1/ (sigma * sigma)
  sigma~dunif(0,100)
  }
"
writeLines(modelString,con="multregression.txt")

loyn.list <- with(loyn,
                  list(abund=ABUND,dist=logDIST,ldist=logLDIST,area=logAREA,
                       graze=GRAZE,alt=ALT,yr=YR.ISOL,
                       n=nrow(loyn))
                  )
params<- c("beta0","beta.dist","beta.ldist","beta.area","beta.graze",
           "beta.alt","beta.yr","sigma")

burnInSteps = 2000
nChains = 3
numSavedSteps = 50000
thinSteps = 5
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
nIter

```

```{r runJags1,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
loyn.r2jags <- jags(data=loyn.list,
                    inits=NULL,
                    parameters.to.save=params,
                    model.file="multregression.txt",
                    n.chains=nChains,
                    n.iter=nIter,
                    n.burnin=burnInSteps,
                    n.thin=thinSteps
                    )
```

There is a shorter way. Instead of having to write out all the beta's, there is a matrix of beta's. This will work for simple regression, multiple regression etc. It is as follows:

```{r}
modelString="
model {
  for (i in 1:n) {
    y[i]~dnorm(mu[i],tau)
    mu[i] <- inprod(beta[],x[i,])
    y.err[i] <- y[i] - mu[i]
    }
  for (i in 1:nX) {
    beta[i] ~ dnorm(0,1.0E-06)
    }
  tau <- 1/ (sigma*sigma)
  sigma~dunif(0,100)
  }
"
writeLines(modelString,con="multregression1.txt")
```

beta[] works out how many betas you need and makes a matrix of them. Then we can just look through defining our priors for teh betas!

So the data list we supply must have:
- the resonse vector
- a model matrix of the predictor variables (linear predictor)
- N (the number of rows in teh dataset)
- nX, the number of predictors (including the intercept)

Her is the model matrix. The first column contains only ones, the other columns represent the predictor variables

```{r}
X <- model.matrix(~logDIST+logLDIST+logAREA+GRAZE+ALT+YR.ISOL,data=loyn)
head(X)
```

Here, X is a matrix!

Now I will count how many columns there are in this model matrix. In this case this is easy - there are seven. But it is useful to have this calculated for use for some circumstances
```{r}
nX <- ncol(X)
nX
```

Now I have to define the parameters and run jags

```{r}
params<- c("beta","sigma","y.err")
burnInSteps = 2000
nChains = 3
numSavedSteps = 50000
thinSteps = 5
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
nIter

loyn.list <- with(loyn,list(y=ABUND, x=X, nX=nX, n=nrow(loyn)))
```
```{r runJags,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
loyn.r2jags2 <- jags(data=loyn.list,
                    inits=NULL,
                    parameters.to.save=params,
                    model.file="multregression1.txt",
                    n.chains=nChains,
                    n.iter=nIter,
                    n.burnin=burnInSteps,
                    n.thin=thinSteps
                    )
```

Now, we can generate the traceplots and decide whether we had enough iterations and appropriate thinning. Were the chains well mixed? Is the posterior likely to be stable?

```{r}
#quartz()
plot(as.mcmc(loyn.r2jags2))
```

So now I am specifically indicating only to plot columns 1 to 7, which I know are the intercept and the partial slopes.

```{r}
plot(as.mcmc(loyn.r2jags2$BUGSoutput$sims.matrix[,1:7]))   
```

BTW, the parameters are normally arranged in alphabetical order, so it is a good idea to make sure all the one you are interested in (e.g. beta) are before other studd alphabetically. JAGS also throws in Deviance, but this is usually put in at the end.

Check for autocorrelation

```{r}
autocorr.diag(as.mcmc(loyn.r2jags2$BUGSoutput$sims.matrix[,c(1:7,9)]))
```

No evidence of autocorrelation. 

Now we look at the parameters of the estimates and confidence intervals etc
```{r}
#print(loyn.r2jags)
```


The first beta will be the intercept. After that, you need to remember what order you put the predictors in the model.matrix. Recall 

```
X <- model.matrix(~logDIST+logLDIST+logAREA+GRAZE+ALT+YR.ISOL,data=loyn)
head(X)
```


--------------------------
ANOVA

Fit a single factor ANOVA for the day data modeling the number of newly recruited barnacles against TREAT (substrate type - Algae1, Algae2, Naturally bare and Scraped bare).

```{r}
day<-(read.csv('day.csv',strip.white=T))
head(day)
summary(lm(BARNACLE~TREAT,data=day))
```

Now do a Tukeys test(post-hoc pairwise comparisons). Take note of the power reductions as a result of the Tukey's test

```{r,include=FALSE}
library(multcomp)
```
```{r}
summary(glht(day.lm,linfct=mcp(TREAT="Tukey")))
```

Recall that the intercept represents the mean of the fisrt treatment group 9the mean number of barnacles on the algae1 substrate)
TREATALT2 regpresents the effect (difference between the number of barnacles on the algae2 surface vs the number on algae1).
Similary, TREATNB is the difference between NB and ALG1 and so on....

The Tukeys test suggests that there is insufficient evidence to be able to conclude there is a difference (effect) between ALG2 and ALG1. 

Now lets have a go at this in a Bayesian framework.
```{r}
modelString="
model {
  for (i in 1:n) {
    barnacle[i]~dnorm(mean[i],tau.res)
    mean[i] <- alpha+beta[treat[i]]
    }
#priors and derivatives
    alpha~dnorm(0,1.0E-6)
    beta[1] <- 0
  for (i in 2:nTreat) {
    beta[i] ~ dnorm(0,1.0E-06) #prior
    }
  tau.res <- 1/ (sigma.res*sigma.res)
  sigma.res~dgamma(0.001,0.001) #prior on sd residulas
  }
"
day.list <- with(day,
                 list(barnacle=BARNACLE,
                      treat=as.numeric(TREAT),
                      n=nrow(day),
                      nTreat=length(levels(TREAT))
                      )
                 )

day.list

writeLines(modelString,con="anova.txt")
```

Note that the differece is the way that beta is defined for categorical variables
beta[treat[i]] rather than beta*treat[i]

Since alpha is the intercept (which I just indicated was the mean of the first treatment), we need to set the first effect prior to zero.. beta[1] <- 0

What data do we need to supply to JAGS?
- barnacle - the response vector
- treat - the categorical vector (HOWEVER MUST BE NUMBERS, NOT WORDS)
- n - the number of rows in the data set.
- nTreat - the number of treatments

Notice that the TREAT categorical variable is now integers.

Now define the parameters etc that you want to get back from JAGS as well as the MCMC parameters (number of iterations etc)

```{r}
params <- c("alpha","beta","sigma.res")
burInSteps = 200
nChains =3
numSavedSteps = 5000
thinSteps = 10
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
```

```{r runJags5,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
day.r2jags1<- jags(data=day.list,
                    inits=NULL,
                    parameters.to.save=params,
                    model.file="anova.txt",
                    n.chains=nChains,
                    n.iter=nIter,
                    n.burnin=burnInSteps,
                    n.thin=thinSteps
                    )
```

Check the traceplots and autocorrelation.

```{r}
plot(as.mcmc(day.r2jags1))
```

Note that we set beta[1] to zero, so we ignore that one. The others all look fine to me, althoug it would not have hurt to run a few more iterations

```{r}
autocorr.diag(as.mcmc(day.r2jags1))
```

Lag 10 looks fine, no correlations over 0.2.

Chain si well mized, so the samples are likely to reflect the posterior output well.
Have a quikc look at the parameter estimates and we will compare them to the frequentist ANOVA.

```
print(day.r2jags1)
```

Pretty similar
alpha = intercept = mean of ALG1
beta[2] = TEATALG2 = ALG2 - ALG1
beta[3] = TREATNB = NB - ALG1

So how about the explore other comparisons. What about we compare each group to each toher group.  What about also exploring specific comparisons such as the average of treatment (NB and S) vs (ALG2 and ALG1)

We can either do this inside of JAGS, or outside of JAGS.

Lets start withinside of JAGS, we just have to make a couple of modifications to the JAGS model...

```{r}
modelString="
model {
  for (i in 1:n) {
    barnacle[i]~dnorm(mean[i],tau.res)
    mean[i] <- alpha+beta[treat[i]]
    }
#priors and derivatives
    alpha~dnorm(0,1.0E-6)
    beta[1] <- 0
  for (i in 2:nTreat) {
    beta[i] ~ dnorm(0,1.0E-06) #prior
    }
  tau.res <- 1/ (sigma.res*sigma.res)
  sigma.res~dgamma(0.001,0.001) #prior on sd residuals
#Group mean derivatives
  for (i in 1:nTreat) {
    Treatment.means[i] <- beta[i]+alpha
}
#pairwise effects
for (j in 1:nTreat){
for (i in 1:nTreat){
eff[j,i] <- Treatment.means[j] - Treatment.means[i]
}
}
#Bare vs Algae
comp1 <- ((Treatment.means[3]+Treatment.means[4])/2) - ((Treatment.means[1]+Treatment.means[2])/2)
}
"
writeLines(modelString,con="anova2.txt")
```

Firstly, we are calculating the mean of each group from the effects. Since effects are expressed relative to the first group, each of the treatment means must just be the effect (beta) plu the intercept (first groups mean)

Once you have the treatment means, you can come up with any comparison. The first set that is created (after the #pairwise effects) is a double loop so as to compare each treatment with each other treatment (yes, that will mean we are comparing ALG1 with ALG1 which is obviously pointless and we will aslo have ALG1 vc ALG2 and ALG2 vs ALG1 which are obviously the same thing, but is doesn't really cost anything and it makes the code a little shorter)

Finally, we have a planned comparison (contrast) comparing the average of the bare surfaces to the average of the bare surfaces to the average of the algae surfaces (and we have called this comparison comp1\                                                                                                                                                                    )

following stays the same:
```
day.list <- with(day,
                 list(barnacle=BARNACLE,
                      treat=as.numeric(TREAT),
                      n=nrow(day),
                      nTreat=length(levels(TREAT))
                      )
                 )
burInSteps = 200
nChains =3
numSavedSteps = 5000
thinSteps = 10
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
```

Add the new stuff we want to get from JAGS
```{r}
params <- c("alpha","beta","sigma.res","Treatment.means","eff","comp1")
```

```{r runJags6,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
day.r2jags2<- jags(data=day.list,
                    inits=NULL,
                    parameters.to.save=params,
                    model.file="anova2.txt",
                    n.chains=nChains,
                    n.iter=nIter,
                    n.burnin=burnInSteps,
                    n.thin=thinSteps
                    )
```

Check the traceplots and autocorrelation.

```{r}
plot(as.mcmc(day.r2jags2))
```

Note that we set beta[1] to zero, so we ignore that one. The others all look fine to me, althoug it would not have hurt to run a few more iterations

```{r}
autocorr.diag(as.mcmc(day.r2jags2))
```

The key here is generating the treatment means (Treatment.means [1] etc)

Once you have 4941 (in this case) samples of each of the treatemetn you might as well treat them as the pplation values.
After that, all things are simple...

Imagine you had four colums in a spreadsheet each representing the population values for the four treatments. If you wanted to know whether treatment 1 was different to treatment 2, you would just calulate the means of both and compare them (taking into consideration their confidence intercals). The point is, once you hace those samples, everything is simple.

Tale a look at some of the pairwise comparisons.
eff[1,2] represents ALG1 vs ALG2
eff[2,3] represents ALG2 vs NB
etc and no loss of powe.

We can compare as many as we have sensible hypotheses about. No adjustments required. comp1 represents Bare vs Algae substrates.

Hypothesis tests are then based on credibility intervals (do they include 0 or not)

We could have fit he anova model using the regression Bayesian model that we defined earlier - the one with the inprod.
The model matrix we would supply to it is

```{r}
X <- model.matrix(~TREAT,data=day)
nX <- ncol(X)
day.list <- list(y=day$BARNACLE, x=X, nX=nX, n=nrow(day))
```

All else would be the same.
```{r runJags7,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
day.r2jags3<- jags(data=day.list,
                    inits=NULL,
                    parameters.to.save=c('beta','sigma'),
                    model.file="multregression1.txt",
                    n.chains=nChains,
                    n.iter=nIter,
                    n.burnin=burnInSteps,
                    n.thin=thinSteps
                    )
```

Look at X (the model.matrix), see how it's dummy coded.
```{r}
head(X)
```

The output is again very similar to the frequentist.

That one model (multregression) can be used for any multiple regression or anova (including factorial anova or ancova)

Last thing, lets modify it for poisson regression. 

```{r}
modelString="
model {
  for (i in 1:n) {
    y[i]~dpois(mu[i])
    log(mu[i]) <- inprod(beta[],x[i,])
    y.err[i] <- y[i] - mu[i]
    }
for (i in 1:nX) {
    beta[i] ~ dnorm(0,1.0E-06) #prior
    }
}
"
writeLines(modelString,con="poissonregression.txt")
```

Notice that we no longer have tau, since the poisson distribution is only defined by mu (the center). Instead of mu <- inprod() we have log(mu[i]) <- inprod
The log is the link fundtion (appropriate for poisson)

Then just re-run jags (TAKE OUT SIGMA from the parameters.to.save)
Note, it will be a bit slower and technically, we should have increased the number of burnins to 50%

Also remember that the parameter estimates are on a log scale, but can easily be back-transformed using exp()

```{r runJags8,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
day.r2jags3<- jags(data=day.list,
                    inits=NULL,
                    parameters.to.save=c('beta'),
                    model.file="poissonregression.txt",
                    n.chains=nChains,
                    n.iter=nIter,
                    n.burnin=burnInSteps,
                    n.thin=thinSteps
                    )
```

Actually, what you should do is first calculate the treatment means (eg. exp(beta[1]) is the mean of ALG1, treatment mean of algae 2 = exp(beta[2] +beta[1])
That is, do the addition and make the back transform the last operation.

But otherwise, glm is not any more difficult that lm in Bayesian. 


For interaction terms:
X <- model.matrix(~AREA*DIST, data = loyn)
beta[1] is intercept, etc, relate to the column names of X
```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```


```{r}

```





