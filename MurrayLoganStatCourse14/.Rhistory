```{r}
print(fert.r2jags)
```
In this, you want n.eff to be at least half of the number of iterations. Because there is no use of calculating the mean slopes etc when you don't have a high enough number of **effective** estimates.
The model shown before was the bare minimum required. Let's make it a little more complicated. We are now storing the residuals (y.err) and something that test  the probability of the slope being greater than 0.
```{r}
modelString="
model {
#Likelihood
for (i in 1:n) {
y[i]~dnorm(mu[i],tau)
mu[i] <- beta0+beta1*x[i]
y.err[i] <- x[i] - mu[i]
}
#Priors
beta0 ~dnorm(0,1.0E-6)
beta1 ~dnorm(0,1.0E-6)
tau <- 1/ (sigma * sigma)
sigma~dunif(0,100)
p.increase <- step(beta1)
sd.x <- abs(beta1)*sd(x[])
sd.resid <- sd(y.err)
}
"
writeLines(modelString,con="regression2.txt")
fert.list <- with(fert,
list(x=FERTILIZER,
y=YIELD,n=nrow(fert))
)
```
We need to change the parameters, because we want more to be returned to us
```{r}
params<- c("beta0","beta1","sigma","p.increase","sd.x","sd.resid")
```
```{r}
burnInSteps = 2000
nChains = 3
numSavedSteps = 50000
thinSteps = 5
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
nIter
```
```{r runJags2,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
fert.r2jags3 <- jags(data=fert.list,
inits=NULL,
parameters.to.save=params,
model.file="regression2.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
```
```{r printJags}
#print(fert.r2jags3)
```
Start making some plots.
First convert to mcmc output:
```{r}
fert.mcmc <- as.mcmc(fert.r2jags$BUGSoutput$sims.matrix)
head(fert.mcmc)
```
First step, identify the columns in which the steps and intercept are in: bet0 and beta1, first 2 columns.
We now have 50000 coefficients for each the slope and the intercept.
```{r}
coefs <- fert.mcmc[,1:2]
xs <- seq(min(fert$FERTILIZER), max(fert$FERTILIZER),len=100)
Xmat <- model.matrix(~FERTILIZER,data=data.frame(FERTILIZER=xs))
head(Xmat)
pred <- coefs %*% t(Xmat)
dim(pred)
```
Now calculate the average of all the predictions
```{r}
#newdata <- adply(pred,2,function(x){
#  data.frame(Mean=mean(x),Median=median(x),HPDinterval(as.mcmc(x))
#})
#newdata <- cbind(newdata,FERTILIZER=xs)
#head(newdata)
```
Now do the plotting
```{r}
#ggplot(newdata,aes(y=Mean,x=FERTILIZER)) + geom_line()
#  geom_ribbon(aes(ymin=lower,ymax=upper),fill='blue', alpha=.2) +
# theme_classic()
```
```{r}
loyn <- read.csv('loyn.csv',strip.white=T)
head(loyn)
loyn$logAREA <- log10(loyn$AREA)
loyn$logDIST <- log10(loyn$DIST)
loyn$logLDIST <- log10(loyn$LDIST)
summary(lm(ABUND~logAREA+logDIST+logLDIST+GRAZE+ALT+YR.ISOL,data=loyn))
```
```{r}
modelString="
model {
#Likelihood
for (i in 1:n) {
abund[i]~dnorm(mu[i],tau)
mu[i] <- beta0+beta.dist*(dist[i])+beta.ldist*(ldist[i]) +
beta.area*(area[i]) + beta.graze*(graze[i]) +
beta.alt*(alt[i]) + beta.yr*(yr[i])
}
#Priors
beta0 ~dnorm(0,1.0E-6)
beta.dist ~dnorm(0,1.0E-6)
beta.ldist ~dnorm(0,1.0E-6)
beta.area ~dnorm(0,1.0E-6)
beta.graze ~dnorm(0,1.0E-6)
beta.alt ~dnorm(0,1.0E-6)
beta.yr ~dnorm(0,1.0E-6)
tau <- 1/ (sigma * sigma)
sigma~dunif(0,100)
}
"
writeLines(modelString,con="multregression.txt")
loyn.list <- with(loyn,
list(abund=ABUND,dist=logDIST,ldist=logLDIST,area=logAREA,
graze=GRAZE,alt=ALT,yr=YR.ISOL,
n=nrow(loyn))
)
params<- c("beta0","beta.dist","beta.ldist","beta.area","beta.graze",
"beta.alt","beta.yr","sigma")
burnInSteps = 2000
nChains = 3
numSavedSteps = 50000
thinSteps = 5
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
nIter
```
```{r runJags1,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
loyn.r2jags <- jags(data=loyn.list,
inits=NULL,
parameters.to.save=params,
model.file="multregression.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
```
There is a shorter way. Instead of having to write out all the beta's, there is a matrix of beta's. This will work for simple regression, multiple regression etc. It is as follows:
```{r}
modelString="
model {
for (i in 1:n) {
y[i]~dnorm(mu[i],tau)
mu[i] <- inprod(beta[],x[i,])
y.err[i] <- y[i] - mu[i]
}
for (i in 1:nX) {
beta[i] ~ dnorm(0,1.0E-06)
}
tau <- 1/ (sigma*sigma)
sigma~dunif(0,100)
}
"
writeLines(modelString,con="multregression1.txt")
```
beta[] works out how many betas you need and makes a matrix of them. Then we can just look through defining our priors for teh betas!
So the data list we supply must have:
- the resonse vector
- a model matrix of the predictor variables (linear predictor)
- N (the number of rows in teh dataset)
- nX, the number of predictors (including the intercept)
Her is the model matrix. The first column contains only ones, the other columns represent the predictor variables
```{r}
X <- model.matrix(~logDIST+logLDIST+logAREA+GRAZE+ALT+YR.ISOL,data=loyn)
head(X)
```
Here, X is a matrix!
Now I will count how many columns there are in this model matrix. In this case this is easy - there are seven. But it is useful to have this calculated for use for some circumstances
```{r}
nX <- ncol(X)
nX
```
Now I have to define the parameters and run jags
```{r}
params<- c("beta","sigma","y.err")
burnInSteps = 2000
nChains = 3
numSavedSteps = 50000
thinSteps = 5
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
nIter
loyn.list <- with(loyn,list(y=ABUND, x=X, nX=nX, n=nrow(loyn)))
```
```{r runJags,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
loyn.r2jags2 <- jags(data=loyn.list,
inits=NULL,
parameters.to.save=params,
model.file="multregression1.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
```
Now, we can generate the traceplots and decide whether we had enough iterations and appropriate thinning. Were the chains well mixed? Is the posterior likely to be stable?
```{r}
#quartz()
plot(as.mcmc(loyn.r2jags2))
```
So now I am specifically indicating only to plot columns 1 to 7, which I know are the intercept and the partial slopes.
```{r}
plot(as.mcmc(loyn.r2jags2$BUGSoutput$sims.matrix[,1:7]))
```
BTW, the parameters are normally arranged in alphabetical order, so it is a good idea to make sure all the one you are interested in (e.g. beta) are before other studd alphabetically. JAGS also throws in Deviance, but this is usually put in at the end.
Check for autocorrelation
```{r}
autocorr.diag(as.mcmc(loyn.r2jags2$BUGSoutput$sims.matrix[,c(1:7,9)]))
```
No evidence of autocorrelation.
Now we look at the parameters of the estimates and confidence intervals etc
```{r}
#print(loyn.r2jags)
```
The first beta will be the intercept. After that, you need to remember what order you put the predictors in the model.matrix. Recall
```
X <- model.matrix(~logDIST+logLDIST+logAREA+GRAZE+ALT+YR.ISOL,data=loyn)
head(X)
```
--------------------------
ANOVA
Fit a single factor ANOVA for the day data modeling the number of newly recruited barnacles against TREAT (substrate type - Algae1, Algae2, Naturally bare and Scraped bare).
```{r}
day<-(read.csv('day.csv',strip.white=T))
head(day)
summary(lm(BARNACLE~TREAT,data=day))
```
Now do a Tukeys test(post-hoc pairwise comparisons). Take note of the power reductions as a result of the Tukey's test
```{r,include=FALSE}
library(multcomp)
```
```{r}
summary(glht(day.lm,linfct=mcp(TREAT="Tukey")))
```
Recall that the intercept represents the mean of the fisrt treatment group 9the mean number of barnacles on the algae1 substrate)
TREATALT2 regpresents the effect (difference between the number of barnacles on the algae2 surface vs the number on algae1).
Similary, TREATNB is the difference between NB and ALG1 and so on....
The Tukeys test suggests that there is insufficient evidence to be able to conclude there is a difference (effect) between ALG2 and ALG1.
Now lets have a go at this in a Bayesian framework.
```{r}
modelString="
model {
for (i in 1:n) {
barnacle[i]~dnorm(mean[i],tau.res)
mean[i] <- alpha+beta[treat[i]]
}
#priors and derivatives
alpha~dnorm(0,1.0E-6)
beta[1] <- 0
for (i in 2:nTreat) {
beta[i] ~ dnorm(0,1.0E-06) #prior
}
tau.res <- 1/ (sigma.res*sigma.res)
sigma.res~dgamma(0.001,0.001) #prior on sd residulas
}
"
day.list <- with(day,
list(barnacle=BARNACLE,
treat=as.numeric(TREAT),
n=nrow(day),
nTreat=length(levels(TREAT))
)
)
day.list
writeLines(modelString,con="anova.txt")
```
Note that the differece is the way that beta is defined for categorical variables
beta[treat[i]] rather than beta*treat[i]
Since alpha is the intercept (which I just indicated was the mean of the first treatment), we need to set the first effect prior to zero.. beta[1] <- 0
What data do we need to supply to JAGS?
- barnacle - the response vector
- treat - the categorical vector (HOWEVER MUST BE NUMBERS, NOT WORDS)
- n - the number of rows in the data set.
- nTreat - the number of treatments
Notice that the TREAT categorical variable is now integers.
Now define the parameters etc that you want to get back from JAGS as well as the MCMC parameters (number of iterations etc)
```{r}
params <- c("alpha","beta","sigma.res")
burInSteps = 200
nChains =3
numSavedSteps = 5000
thinSteps = 10
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
```
```{r runJags5,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
day.r2jags1<- jags(data=day.list,
inits=NULL,
parameters.to.save=params,
model.file="anova.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
```
Check the traceplots and autocorrelation.
```{r}
plot(as.mcmc(day.r2jags1))
```
Note that we set beta[1] to zero, so we ignore that one. The others all look fine to me, althoug it would not have hurt to run a few more iterations
```{r}
autocorr.diag(as.mcmc(day.r2jags1))
```
Lag 10 looks fine, no correlations over 0.2.
Chain si well mized, so the samples are likely to reflect the posterior output well.
Have a quikc look at the parameter estimates and we will compare them to the frequentist ANOVA.
```
print(day.r2jags1)
```
Pretty similar
alpha = intercept = mean of ALG1
beta[2] = TEATALG2 = ALG2 - ALG1
beta[3] = TREATNB = NB - ALG1
So how about the explore other comparisons. What about we compare each group to each toher group.  What about also exploring specific comparisons such as the average of treatment (NB and S) vs (ALG2 and ALG1)
We can either do this inside of JAGS, or outside of JAGS.
Lets start withinside of JAGS, we just have to make a couple of modifications to the JAGS model...
```{r}
modelString="
model {
for (i in 1:n) {
barnacle[i]~dnorm(mean[i],tau.res)
mean[i] <- alpha+beta[treat[i]]
}
#priors and derivatives
alpha~dnorm(0,1.0E-6)
beta[1] <- 0
for (i in 2:nTreat) {
beta[i] ~ dnorm(0,1.0E-06) #prior
}
tau.res <- 1/ (sigma.res*sigma.res)
sigma.res~dgamma(0.001,0.001) #prior on sd residuals
#Group mean derivatives
for (i in 1:nTreat) {
Treatment.means[i] <- beta[i]+alpha
}
#pairwise effects
for (j in 1:nTreat){
for (i in 1:nTreat){
eff[j,i] <- Treatment.means[j] - Treatment.means[i]
}
}
#Bare vs Algae
comp1 <- ((Treatment.means[3]+Treatment.means[4])/2) - ((Treatment.means[1]+Treatment.means[2])/2)
}
"
writeLines(modelString,con="anova2.txt")
```
Firstly, we are calculating the mean of each group from the effects. Since effects are expressed relative to the first group, each of the treatment means must just be the effect (beta) plu the intercept (first groups mean)
Once you have the treatment means, you can come up with any comparison. The first set that is created (after the #pairwise effects) is a double loop so as to compare each treatment with each other treatment (yes, that will mean we are comparing ALG1 with ALG1 which is obviously pointless and we will aslo have ALG1 vc ALG2 and ALG2 vs ALG1 which are obviously the same thing, but is doesn't really cost anything and it makes the code a little shorter)
Finally, we have a planned comparison (contrast) comparing the average of the bare surfaces to the average of the bare surfaces to the average of the algae surfaces (and we have called this comparison comp1\                                                                                                                                                                    )
following stays the same:
```
day.list <- with(day,
list(barnacle=BARNACLE,
treat=as.numeric(TREAT),
n=nrow(day),
nTreat=length(levels(TREAT))
)
)
burInSteps = 200
nChains =3
numSavedSteps = 5000
thinSteps = 10
nIter = ceiling((numSavedSteps * thinSteps)/nChains)
```
Add the new stuff we want to get from JAGS
```{r}
params <- c("alpha","beta","sigma.res","Treatment.means","eff","comp1")
```
```{r runJags6,cache=TRUE}
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
day.r2jags2<- jags(data=day.list,
inits=NULL,
parameters.to.save=params,
model.file="anova2.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
```
Check the traceplots and autocorrelation.
```{r}
plot(as.mcmc(day.r2jags2))
```
Note that we set beta[1] to zero, so we ignore that one. The others all look fine to me, althoug it would not have hurt to run a few more iterations
```{r}
autocorr.diag(as.mcmc(day.r2jags2))
```
The key here is generating the treatment means (Treatment.means [1] etc)
Once you have 4941 (in this case) samples of each of the treatemetn you might as well treat them as the pplation values.
After that, all things are simple...
Imagine you had four colums in a spreadsheet each representing the population values for the four treatments. If you wanted to know whether treatment 1 was different to treatment 2, you would just calulate the means of both and compare them (taking into consideration their confidence intercals). The point is, once you hace those samples, everything is simple.
Tale a look at some of the pairwise comparisons.
eff[1,2] represents ALG1 vs ALG2
eff[2,3] represents ALG2 vs NB
etc and no loss of powe.
We can compare as many as we have sensible hypotheses about. No adjustments required. comp1 represents Bare vs Algae substrates.
Hypothesis tests are then based on credibility intervals (do they include 0 or not)
We could have fit he anoca model using the regression Bayesian model that we defined earlier - the one with the inprod.
The model matrix we would supply to it is
```{r}
X <- model.matrix(~TREAT,data=day)
nX <- ncol(X)
day.list <- list(y=day$BARNACLE, x=X, nX=nX, N=nrow(day))
```
All else would be the same.
```{r}
```
```{r}
```
```{r}
```
```{r}
```
```{r}
```
```{r}
```
```{r}
```
```{r}
```
```{r}
```
```{r}
```
day.r2jags3<- jags(data=day.list,
inits=NULL,
parameters.to.save=c('beta','sigma'),
model.file="anova2.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
```
day.r2jags3<- jags(data=day.list,
inits=NULL,
parameters.to.save=c('beta','sigma'),
model.file="multregression1.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
day.list <- list(y=day$BARNACLE, x=X, nX=nX, n=nrow(day))
day.r2jags3<- jags(data=day.list,
inits=NULL,
parameters.to.save=c('beta','sigma'),
model.file="multregression1.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
```
head(X)
modelString="
model {
for (i in 1:n) {
y[i]~dpois(mu[i])
log(mu[i] <- inprod(beta[],x[i,])
y.err[i] <- y[i] - mu[i]
}
for (i in 1:nX) {
beta[i] ~ dnorm(0,1.0E-06) #prior
}
}
"
writeLines(modelString,con="poissonregression.txt")
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
day.r2jags3<- jags(data=day.list,
inits=NULL,
parameters.to.save=c('beta'),
model.file="poissonregression.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
modelString="
model {
for (i in 1:n) {
y[i]~dpois(mu[i])
log(mu[i]) <- inprod(beta[],x[i,])
y.err[i] <- y[i] - mu[i]
}
for (i in 1:nX) {
beta[i] ~ dnorm(0,1.0E-06) #prior
}
}
"
writeLines(modelString,con="poissonregression.txt")
library("R2jags", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
day.r2jags3<- jags(data=day.list,
inits=NULL,
parameters.to.save=c('beta'),
model.file="poissonregression.txt",
n.chains=nChains,
n.iter=nIter,
n.burnin=burnInSteps,
n.thin=thinSteps
)
